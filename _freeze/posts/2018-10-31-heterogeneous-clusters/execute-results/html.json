{
  "hash": "7d8a611c7f28217b1226c7ec02ac4f64",
  "result": {
    "markdown": "---\ntitle: Cluster randomized trials can be biased when cluster sizes are heterogeneous\nauthor: 'DeclareDesign Team'\ndate: '2018-10-31'\nslug: bias-cluster-randomized-trials\noutput:\n  html_document:\n    highlight: tango\n    theme: cerulean\n    code_folding: show\nbibliography: bib/blog.bib  \n---\n\n\n\n\nIn many experiments, random assignment is performed at the level of clusters. Researchers are conscious that in such cases they cannot rely on the usual standard errors and they should take account of this feature by clustering their standard errors. Another, more subtle, risk in such designs is that if clusters are of different sizes, clustering can actually introduce bias, *even if all clusters are assigned to treatment with the same probability*. Luckily, there is a relatively simple fix that you can implement at the design stage.\n\nFor intuition, imagine two clusters, one of size 1,000,000 and the other of size 1. Say that outcomes are 0 in both clusters in the control condition, but that in the treatment condition they are 0 in the big cluster and 1 in the small one. Then the average treatment effect is about 0 (really: 1/1,000,000). But depending on which cluster is assigned to treatment one will (using difference-in-means) estimate either 0 or 1. So the expected estimate is 0.5. Far from the truth.\n\nThe Horvitz-Thompson estimator is an alternative to difference-in-means and does not have this problem. Using Horvitz-Thompson, one would estimate either $\\frac{1}{10^6}\\left(\\frac{1}{0.5} - \\frac{0}{0.5}\\right)$ or $\\frac{1}{10^6}\\left(\\frac{0}{0.5} - \\frac{0}{0.5}\\right)$ and so get it right in expectation. In practice however, researchers often avoid Horvitz-Thompson since it can produce estimates outside of the ranges of the data and can exhibit high variance.\n\nThe design-based fix for this problem is given in @imai2009essential. As they show, the problem can be greatly alleviated by blocking on cluster size. We will use a simple design declaration to show the problem and how blocking helps.\n\n# A design with heterogeneous cluster sizes\n\nLet's declare a design with heterogeneous cluster sizes. There are 300 units in 12 clusters. Two bigger clusters are of size 100 and 10 smaller clusters are of size 10. The 200 units in clusters of size 100 have a 0 treatment effect, the other 100 in clusters of size 10 have an effect of 3. This means that the average treatment effect is 1. Note that we did not include any cluster level \"shocks\" though we did include heterogeneous effects by cluster. \n\nHere is the design:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN_clusters <- 12\n\ncluster_design <- \n  # M: Model\n  declare_model(clusters = add_level(N = N_clusters, \n                                          cl_size = rep(c(100, 10), c(N/6, N - N/6)),\n                                          effect = ifelse(cl_size == 100, 0, 3)),\n                     units    = add_level(N = cl_size, u = rnorm(N, sd = .2),\n                                          Y_Z_0 = u, Y_Z_1 = u + effect)) +\n  \n  # I: Inquiry\n  declare_inquiry(ATE_i = mean(Y_Z_1 - Y_Z_0)) +\n\n  # D: Data Strategy\n  declare_assignment(Z = cluster_ra(clusters = clusters)) +\n  \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n\n  # A: Answer Strategy\n  declare_estimator(Y ~ Z, inquiry = \"ATE_i\", clusters = clusters,\n                    model = lm_robust, label = \"dim\") +\n  \n  declare_estimator(Y ~ Z, inquiry = \"ATE_i\", clusters = clusters,\n                    condition_prs = 0.5, simple = FALSE,\n                    model = horvitz_thompson, label = \"ht\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nIn the plot below we show the distribution of possible estimates from different possible random assignments. The true treatment effect is 1. We see bias (of size 0.327) from the fact that the distribution is clearly not centered at 1. Very large effects (approx. 3) are estimated in those cases where both of the large clusters get assigned to control (and so all treated outcomes are around 3, right mode) whereas the estimated effects (approx. 0) when both are assigned to treatment are not so small in comparison (left mode), producing right skew. The HT estimator, in contrast, does fine on bias in this example and also has a tighter distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_design(cluster_design) %>%\n  ggplot(aes(estimate)) + \n  geom_histogram(bins = 30) + \n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  facet_wrap( ~ estimator)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](2018-10-31-heterogeneous-clusters_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Scale matters\n\nThe problem in this design, we saw, arises when all the large units get assigned to the control condition. This unlucky combination is a higher probability event in small studies than it is in large studies. In larger studies we are more likely to see balance in the allocation of units to treatment and control. To illustrate, we scale up to 120 clusters rather than 12. We see a more continuous distribution of treatment effects and in a particular a shift away from the extremes of the distribution, which arise only when like types end up in like conditions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_design(redesign(cluster_design, N_clusters = 120)) %>%\n  ggplot(aes(estimate)) + \n  geom_histogram(bins = 30) + \n  facet_wrap( ~ estimator)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](2018-10-31-heterogeneous-clusters_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nIn this case the bias is of size 0.027, which is a big reduction.\n\n# Block on cluster size to address this risk\n\nHow can we address the bias in the difference-in-means estimator? As described in @imai2009essential, blocking our treatment assignment such that a similar number of large clusters are assigned to treatment and control, and a similar number of small clusters are assigned to treatment and control, can help a lot. \n\nTo see this, we can transform this cluster design into a blocked cluster design by changing the assignment strategy; here using one in which we pair off the clusters based on size and randomly assign one in each pair to treatment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatched_cluster_design <- replace_step(\n  cluster_design, step = 3, declare_assignment(Z = block_and_cluster_ra(clusters = clusters, blocks = cl_size)))\n```\n:::\n\n\nThe sampling distribution of the difference-in-means estimators is tight---variation reflects only the random differences between clusters and not the systematic differences. It is, moreover, now equivalent to the HT distribution since there is essentially no differential weighting within blocks.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_design(matched_cluster_design) %>%\n  ggplot(aes(estimate)) + \n  geom_histogram(bins = 30) + \n  facet_wrap( ~ estimator)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](2018-10-31-heterogeneous-clusters_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n# Extension: The problem is aggravated by sampling (but there is a solution in that case also!)\n\nThis problem is amplified when clusters are sampled from a larger population of clusters with equal probability. In this case both the difference-in-means and the Horvitz Thompson estimators will be biased. The intuition is similar to the problem with random assignment: tiny clusters are equally likely to be sampled as very large clusters, and the inclusion of some tiny clusters biases down both estimators when outcomes are a function of cluster size. This bias will exist even when randomization is blocked on cluster size. One intuitive possibility to address this bias is to change the probability of sampling to oversample large clusters and undersample small clusters -- probability-proportional-to-size sampling. @higgins2014benefits proposes such a design. Intuitively, this helps because those tiny clusters that lead to bias are less likely to be selected. @higgins2014benefits demonstrates that the @hansen1943theory estimator is unbiased in this setting. \n\nWhile we don't declare this alternative design in this post we will sign off with a step declaration for the HH estimator which could be useful for addressing this problem in the future.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A step!\nhansen_hurwitz <- function(data){\n  data %>% group_by(clusters) %>% \n      summarize(is_treated = first(Z), cluster_mean = 1/n() * sum(Y)) %>% \n      ungroup %>% \n      summarize(estimate = sum(1/sum(is_treated) * cluster_mean[is_treated]) -\n                           sum(1/sum(!is_treated) * cluster_mean[!is_treated])) %>% \n      as.data.frame\n  }\n```\n:::\n\n\n# References\n\n",
    "supporting": [
      "2018-10-31-heterogeneous-clusters_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}